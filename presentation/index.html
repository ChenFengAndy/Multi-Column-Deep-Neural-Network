<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Multi-Column Deep Neural Network on CIFAR-10 dataset</title>

		<meta name="description" content="A framework for easily creating beautiful presentations using HTML">
		<meta name="author" content="Hakim El Hattab">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="css/reveal.min.css">
		<link rel="stylesheet" href="css/theme/our-solarized.css" id="theme">

		<!-- For syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

        <!--<script>-->
            <!--$(function(file){-->
                <!--$("#includedContent").load(file);-->
            <!--});-->
        <!--</script>-->


		<!-- If the query includes 'print-pdf', include the PDF print sheet -->
		<script>
			if( window.location.search.match( /print-pdf/gi ) ) {
				var link = document.createElement( 'link' );
				link.rel = 'stylesheet';
				link.type = 'text/css';
				link.href = 'css/print/pdf.css';
				document.getElementsByTagName( 'head' )[0].appendChild( link );
			}
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
                    <img width="200" height="200" src="./img/Stemma.png" alt="unifi stemma" >
					<h3 style="color: #586e75;">Università degli Studi di Firenze</h3>
                    <p>Laurea Magistrale in Ingegneria Informatica</p>
                    <p><small>Corso di Apprendimento Automatico</small></p>
                    <br>
					<h2 style="color: #586e75;">Multi-Column Deep Neural Networks for Image Classification</h2>
					<p>
						<small><a href="mailto:matteo.bruni@gmail.com">Matteo Bruni</a> - <a href="mailto:andrearizzo@outlook.com">Andrea Rizzo</a></small>
					</p>
				</section>

                <section>
                    <h2>Contents</h2>
                    <div class="div-left">
                        <ul>
                            <li>primo della 1a lista</li>
                            <li>secondo della 1a lista
                                <ul>
                                    <li>primo della 2a lista</li>
                                    <li>secondo della 2a lista
                                        <ul>
                                            <li>primo della 3a lista</li>
                                        </ul>
                                    </li>
                                    <li>terzo della 2a lista</li>
                                </ul>
                            </li>
                        </ul>
                    </div>

                    <div class="div-right">
                        <ul>
                          <li>primo
                          <li>secondo
                          <li>terzo
                        </ul>
                    </div>
                </section>



                <section>
                    <h2>Multi-column DNN for image classification</h2>
                    <div class="div-left">
                        <ul>
                            <li><b>Detect the image content:</b></li>
                            <ul>
                                <li>e.g. cat, doog, airplain, birt, etc...</li>
                            </ul>
                            <li><b>Using deep learning algorithms:</b></li>
                            <ul>
                                <li>Convolutional Neural Network (CNN)</li>
                            </ul>
                            <li><b>Famous benchmarks:</b></li>
                            <ul>
                                <li>CIFAR-10 and CIFAR-100;</li>
                                <li>MNIST: handwritten digits;</li>
                            </ul>
                            <li><b>Issues:</b></li>
                            <ul>
                                <li>Higher number of layer than shallow Neural Network (NN);</li>
                                <li>Multiple Deep Neural Network (DNN);</li>
                                <li>Vanishing gradient problem;</li>
                                <li>Computational complexity.</li>
                            </ul>
                        </ul>
                    </div>
                    <div class="div-right" >
                        <div style="min-height: 50px; overflow: hidden"></div>
                        <div class="div-img">
                            <img src="./img/provadnn.png" alt="unifi stemma" >
                        </div>
                    </div>
                </section>

                <section>
                    <h2>Solution proposed</h2>
                    <div class="div-header">
                        It is based on <i><b>Multi-column Deep Neural Networks for Image Classification</b></i>
                        - D. Cireșan, U. Meier and J. Schmidhuber<sup>[1]</sup> of Dalle Molle Institute for Artificial Intelligence (IDSIA).
                    </div>
                    <div>
                        <img src="./img/mcdnn_trasparent.png" alt="unifi stemma" >
                    </div>
                    <div class="div-alone">
                        <ul>
                            <li>Each DNN is iteratively trained over only one processed dataset;</li>
                            <li>The predictions of all DNN are democratically averaged;</li>
                            <li>Softmax layer returns the image's predicted class.</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <section>
                        <h2>Why deep learning?</h2>
                        <div class="div-header">
                            <ul>
                                <li>The visual nervous system proposed by Hubel and Wiesel has a deep hierarchy model<sup>[2]</sup>:</li>
                            </ul>
                        </div>
                        <div class="div-left" style="width: 60%">
                            <ul>
                                <li>Suggests that:</li>
                                <ul>
                                    <li>Concepts are describe in <b>hierarchical</b> ways;</li>
                                    <li><b>Multiple levels architecture</b>:</li>
                                    <ul>
                                        <li>multiple stages of transformation and representation;</li>
                                        <li>with increasing level of abstraction;</li>
                                    </ul>
                                    <li><b>Distributed representation</b>:</li>
                                    <ul>
                                        <li>Each level of abstraction forms a large number of features not mutually exclusive;</li>
                                    </ul>
                                    <li>First extract low-level features i.e. low level concepts</li>
                                    <li>Detect the most frequent patterns i.e. increase abstraction;</li>
                                    <li>Putting all together to identify categories;</li>
                                </ul>
                            </ul>
                        </div>
                        <div class="div-right"  style="width: 40%">
                            <div style="min-height: 80px; overflow: hidden"></div>
                            <div id="div-img">
                                <img src="./img/deep_hierarchy_model.png" alt="unifi stemma" >
                            </div>
                        </div>
                    </section>
                    <section>
                        <h2>Deep learning milestone</h2>
                        <div class="div-alone">
                            <ul>
                                <li>1980: Kunihiko Fukushima introduces the <b>Neocognitron</b><sup>[3]</sup>;</li>
                                <li>1989: Yann LeCun et al. apply backpropagation algorithm to DNN:</li>
                                <ul>
                                    <li>too much time to train the network;</li>
                                </ul>
                                <li>1991: J. Schmidhuber <b>vanishing gradient problem</b>;</li>
                                <li>2000: G. Hinton pre-trained one layer at a time with <b>unsupervised algorithm</b>, then using supervised backpropagation;</li>
                                <li>2010: D. Cireșan by using GPUs, trains a DNN with <b>supervised algorithm</b> even though the vanishing gradient problem;</li>
                                <li>2011: DNN becomes the <b>state-of-art</b> systems in computer vision;</li>
                                <li>2012: D. Ciresan at al. train a Multi-Column DNN and on traffic sign recognition benchmark it <b>out-performs humans</b>.</li>
                            </ul>
                        </div>
                    </section>

                    <section>
                        <h2>Sparse representation</h2>
                        <div class="subtitle">Advantages</div>

                        <div class="div-alone">
                            <ul>
                                <li>Sparse representations advantages:</li>
                                <ul>
                                    <li style="text-align: justify"><b>Information disentangling:</b> a dense representation is highly entangled because almost any change in the input modifies most of
                                    the entries in the representation vector. Instead, if a representation is both sparse and robust to
                                    small input changes, the set of non-zero features is almost always roughly conserved by small
                                    changes of the input;</li>
                                    <li style="text-align: justify"><b>Efficient variable-size representation:</b> different inputs may contain different amounts of information and would be
                                    more conveniently represented using a variable-size data-structure. Varying the number of active neurons allows a model to control the effective
                                    dimensionality of the representation for a given input and the required precision;</li>
                                </ul>
                            </ul>

                        </div>
                    </section>

                    <section>
                        <h2>Sparse representation</h2>
                        <div class="subtitle">Advantages</div>
                        <div class="div-alone">
                            <ul>
                                <li>Sparse representations advantages:</li>
                                <ul>
                                    <li style="text-align: justify"><b>Linear separability:</b> sparse representations are also more likely to be linearly separable, or more
                                    easily separable with less non-linear machinery, simply because the information is represented in
                                    a high-dimensional space.
                                    <!--Besides, this can reflect-->
                                    <!--the original data format. In text-related applications for instance,-->
                                    <!--the original raw data is already very sparse (see Section 4.2).-->
                                    </li>
                                    <li style="text-align: justify"><b> Distributed but sparse:</b> dense distributed representations are the richest representations,
                                        being potentially exponentially more efficient than purely local ones (Bengio, 2009). Sparse representations’ efficiency
                                    is still exponentially greater, with the power of the exponent being the number
                                    of non-zero features. They may represent a good trade-off with respect to the above criteria.</li>
                                </ul>
                            </ul>
                        </div>
                    </section>
                </section>

                <section>
                    <section>
                        <h2>Convolutional Neural Networks (CNN)</h2>
                        <div class="subtitle">Introduction</div>
                        <div class="div-alone">
                            <ul>
                                <li>CNNs are an implementation of deep learning architecture:</li>
                                <ul>
                                    <li>are variations of Multi-Layer Perceptrons (MLP);</li>
                                    <li>each layer has a topological structure, i.e. each unit as a 2D position that corresponds to an image's pixel;</li>
                                </ul>
                                <li>CNN's design:</li>
                                <ul>
                                    <li><b>Convolutional layer</b>: implements the convolution operation as in image processing;</li>
                                    <li><b>Max-pooling layer</b>: implements a form of non-linear down-sampling;</li>
                                    <li><b>Fully connected layer</b>: the upper-layer and corresponds to a traditional MLP.</li>
                                </ul>
                            </ul>
                        </div>
                        <div>
                            <img src="./img/LeNet-trasparent.png">
                            <p class="img-note"><i>Figure: Convolutional Neural Network: 1x48x48-10C2-MP2-16C5-MP2-16C13-180N-7N.</i></p>
                        </div>
                    </section>

                    <section>
                        <h2>Convolutional Neural Networks (CNN)</h2>
                        <div class="subtitle">Convolutional layer</div>
                        <div class="div-alone">
                            <ul>
                                <li>Implements the convolution operation:</li>
                                <ul>
                                    <li>A <b>linear filter</b> $W \in \mathbb{R}^{m \times m}$ is applied to the input image or to a feature map $X \in \mathbb{R}^{N \times N}$. Mathematically:</li>
                                    <div class="equation">$c_{ij}=\sum\limits_{k=0}^{m-1}\sum\limits_{l=0}^{m-1}x_{(i+k)(j+l)}w_{kl}$</div>
                                    <ul>
                                        <li><b>Sparse connectivity</b> i.e. only a subset of units in a feature map are used;</li>
                                    </ul>
                                    <li>After a <b>non-linear function</b> $\sigma$ is applied.</li>
                                    <li>The ouput image $Y = \sigma(C)$ is called <b>feature map</b>;</li>
                                    <ul>
                                        <li>by appling $k$ different filters we obtain $k$ feature maps: $\{Y^{s}\}_{s=1}^{k}$.</li>
                                        <li>$Y \in \mathbb{R}^{(N-m+1) \times (N-m+1)}$</li>
                                    </ul>
                                </ul>
                                <li>We use $k\text{C}n$ to indicate a <b>C-Layer</b> that applies $k$ filters of $n\times n$ dimension to a feature map ;</li>
                            </ul>
                        </div>
                        <div style="padding-top: 5px">
                            <div class="div-left">
                                <img src="./img/cnn_operation.png" width="70%">
                                <p class="img-note"><i>Figure: C-Layer operation.</i></p>
                            </div>

                            <div class="div-right">
                                <img src="./img/convolution.gif" width="70%">
                                <p class="img-note"><i>Figure: convolution operation.</i></p>
                            </div>
                        </div>
                    </section>

                    <section>
                        <h2>Convolutional Neural Networks (CNN)</h2>
                        <div class="subtitle">Convolutional layer: shared weights</div>
                        <div class="div-alone">
                            <ul>
                                <li>The linear filter $W$ is applied across the entire input feature map;</li>
                                <ul>
                                    <li>The units share the same weights $w_{ij}$</li>
                                </ul>
                                <li>Advantage of <b>weight sharing</b>:</li>
                                <ul>
                                    <li>The features che be detected regardless of their position;</li>
                                    <li>Increased of learning efficiency by reducing the number of free parameters;</li>
                                    <li>CNNs achieve better generalization.</li>
                                </ul>
                            </ul>
                        </div>
                        <div >
                            <img style="width: 35%" src="./img/cnn_shared_weights.png">
                            <p class="img-note"><i>Figure: the weiths with the same color are shared.</i></p>
                        </div>
                    </section>

                    <section>
                        <h2>Convolutional Neural Networks (CNN)</h2>
                        <div class="subtitle">Max-pooling layer</div>
                        <div class="div-alone">
                            <ul>
                                <li>M-Layer implements a form of non-linear <b>down-sampling</b>;</li>
                                <li>How <b>M-Layer</b> works:</li>
                                <ul>
                                    <li>The M-Layer receives as input a set of feature maps $I=\{I_j\}$ which is the output of C-Layer that precedes it;</li>
                                    <li>Each feature map $I_j$ is partitioned into a set of non-overlapping rectangles $R_{i}^{(j)} \in \mathbb{R}^{k \times k}$;</li>
                                    <li>For each rectangles $R_{i}^{(j)}$ the maximum value are taken which form the output feature map $O_j$;</li>
                                    <li>The output of M-Layer is a set of feature maps $O=\{O_j\}$;</li>
                                </ul>
                                <li>If the input feature maps $I_j \in \mathbb{R}^{N \times N}$ then the output feature maps $O_j \in \mathbb{R}^{\frac{N}{k} \times \frac{N}{k}} \;$ :</li>
                                <ul>
                                    <li>as each $k \times k$ sub-region is reduced to just a single value via the max function.</li>
                                </ul>
                            </ul>
                        </div>
                                               <div >
                            <br>
                            <img style="width: 40%" src="./img/max_pool.gif">
                            <p class="img-note"><i>Figure: Max-pooling.</i></p>
                        </div>
                    </section>

                    <section>
                        <h2>Convolutional Neural Networks (CNN)</h2>
                        <div class="subtitle">Max-pooling layer: advantage</div>
                        <div class="div-alone">
                            <ul>
                                <li>Advantage of M-Layer:</li>
                                <ul>
                                    <li>reduces the computational complexity for the layers above;</li>
                                    <li>provides a form of translation invariance:</li>
                                    <ul>
                                        <li>If we consider the neighbors of a feature map's unit the 8-connected units;</li>
                                        <li>and max-pooling is done over a $2 \times 2$ region, $3$ out of these $8$ possible configurations will produce exactly the same output at the C-layer above;</li>
                                    </ul>
                                </ul>
                            </ul>
                        </div>
                    </section>

                </section>

                <section>

                    <section>
                        <!--<script type="text/template">-->
                        <h2>Dropout</h2>

                        <div>
                            <b>How to improve a neural network?</b>
                        </div>

                        <div class="div-left">
                            <br>
                            <ul>
                                <li>Train many models and average the models' predictions:</li>
                                <ul>
                                    <li>reduce test error and overfitting;</li>
                                    <li>too expensive for big NN and that already take several days to train.</li>
                                </ul>
                                <li><b>Dropout</b> is a more efficient solution<sup>[4]</sup>. It provides:</li>
                                <ul>
                                    <li>an inexpensive and simple means of both training a large ensemble of models;</li>
                                </ul>
                            </ul>
                        </div>

                        <div class="div-right">
                            <img src="./img/mcdnn_dropout.png" width="80%">
                        </div>

                    </section>
                    <section>
                        <h2>How Dropout works (1)</h2>

                        <div class="div-alone">
                            <ul>
                                <li>Given a feedforward architecture:</li>
                                <ul>
                                    <li>Input: $v \, \in \mathbb{R}^{m}$;</li>
                                    <li>Hidden layers: $ \, h = \{h^{(1)}, \dots, h^{(l)} \}$;</li>
                                    <li>Output: $\, y \in \mathbb{R}^{n}$;</li>
                                </ul>
                                <li>Dropout trains a set of models, each of which:</li>
                                <ul>
                                    <li>contains a subset of the variables in both $v$ and $h$;</li>
                                    <li>each model uses the same set of parameters $\theta$:</li>
                                        <ul>
                                            <li>to parameterize a family of distributions $p(y | v; \theta, \mu)$;</li>
                                            <li>where $\mu \in M$ is a binary mask determining which variables to include in the model;</li>
                                        </ul>
                                </ul>
                            </ul>
                        </div>
                    </section>
                    <section>
                        <h2>How Dropout works (2)</h2>

                        <div class="div-alone">
                            <ul>
                                <li>On each presentation of a training example:</li>
                                <ul>
                                    <li>a different sub-model is obtained by randomly sampling $\mu$;</li>
                                    <li>the sub-model is trained by following the gradient of $log \, p(y | v; \theta, \mu)$;</li>
                                </ul>

                                <li>Each model is trained for only one step and all the models share the parameters $\theta$.</li>
                            </ul>
                            <br><br>
                            <b>Note</b>: each update must have a large effect so that it makes the sub-model induced by that $\mu$ fit the current input $v$ well.
                        </div>
                    </section>

                    <section>
                        <h2>How Dropout works (3)</h2>
                        <div>
                            <b>Model prediction</b>
                        </div>

                        <div class="div-alone">
                            <ul>
                                <li>Model prediction is obtained by averaging together all the sub-models prediction.</li>
                                <li>By using Dropout the number of models trained is exponential:</li>
                                <ul>
                                    <li>when $p(y | v;\theta) = softmax(v^{T}W+b)$</li>
                                    <li>the predictive distribution defined by renormalizing the geometric mean of $p(y | v;\theta, \mu)$ over $M$ is simply given by:</li>
                                    <br>
                                        <p align="center"><b>$softmax(v^{T}\frac{W}{2}+b)$</b></p>

                                </ul>
                            </ul>
                        </div>
                    </section>
                    <section>
                        <h2>Rectified Linear Units (ReLU)</h2>
                        <div class="subtitle">ReLU function</div>
                            Rectified Linear units are a drop in replacement for the traditional nonlinear activation
                            functions.<br>
                            <!--ReLUs (Rectified Linear Units) are capable of having values in the range of-->
                            <!--[0, inf].-->
                        <div class="equation">
                            $ y = max (0 , b + \sum\limits_{i=1}^k x_i  w_i) $ <br>
                        </div>
                            ReLU units are more biologically plausible then the other activation functions, since they
                            model the biological neuron's responses in their area of operation. While sigmoid and tanh
                            activation functions are biologically implausible.<br>

                            <img src="./img/synapse_vs_sigmoid_trasp.png" alt="unifi stemma">
                            <p class="img-note">Figure: left: Firing of a neuron from biological data. Right: traditional activation functions.</p>

                    </section>

                    <section>
                        <h2>Rectified Linear Units (ReLU)</h2>
                        <div class="subtitle">ReLU derivative</div>
                        <div class="div-left">
                            <ul>
                                <li>ReLU derivative:</li>
                                <ul>
                                    <li>is not fully differentiable (not at $0$)</li>
                                    <li>can only take two values, $0$ or $1$.</li>
                                </ul>
                                <br>
                                <li>Compared to the logistic sigmoid neuron:</li>
                                <ul>
                                    <li>it is much more efficient to compute (both its value and its partial derivatives)</li>
                                    <li>considerably speeds up training.</li>
                                    <li>enables much larger network implementations.</li>
                                </ul>
                        </ul>
                        </div>
                        <div class="div-right">
                            <div style="min-height: 50px; overflow: hidden"></div>
                            <img src="./img/relu_trasp.png" width="320" alt="relu">
                            <p class="img-note">Figure: ReLu vs Softplus activation function.</p>
                        </div>


                    </section>

                    <section>
                        <h2>Rectified Linear Units (ReLU)</h2>
                        <div class="subtitle">Potential problem</div>
                        <div class="div-alone">
                            <ul>
                                <li>Potential problem by using ReLU activation function:</li>
                                <ul>
                                    <li>hard saturation at $0$ may hurt optimization by blocking gradient back-propagation.
                                        To evaluate the potential impact of this effect we also investigate the <i>soft-plus activation</i>:
                                        <div class="equation">
                                            $softplus(x) = log(1+ e^x)$
                                        </div>
                                        which is a smooth version of the rectifying non-linearity.
                                    </li>
                                    <li>We lose the exact sparsity, but may hope to gain easier training. However, experimental results tend to contradict that hypothesis, suggesting
                                        that hard zeros can actually help supervised training.</li>
                                </ul>


                            </ul>
                        </div>
                    </section>

                    <section>
                        <h2>Rectified Linear Units (ReLU)</h2>
                        <div class="subtitle">Advantages</div>
                        <div class="div-alone">
                            <ul>
                                <li>The Advantages to use ReLU activation function are:</li>
                                <ul>
                                    <li><b>Biological plausibility:</b> one-sided, compared to the antisymmetry of tanh;</li>
                                    <li><b>Sparse activation:</b> e.g. in a randomly initialized networks, only about 50% of hidden units is activated (having a non-zero output);</li>
                                    <li><b>Efficient gradient propagation:</b> no vanishing gradient problem or exploding effect;</li>
                                    <li><b>Efficient computation:</b> only comparison, addition and multiplication.</li>
                                </ul>
                            </ul>
                        </div>
                    </section>

                    <section>
                        <h2>Maxout</h2>
                        <div class="subtitle">Introduction</div>
                        <div class="div-alone">
                            <ul>
                                <li><b>Maxout</b> is so named because its output is the max of a set of inputs:
                                    <ul>
                                        <li>it is a natural companion to dropout;</li>
                                    </ul>
                                <li>Maxout model introduce a new type of activation function called <b>maxout unit</b><sup>[6]</sup>:</li>
                                    <ul>
                                        <li>Given an input $x \in \mathbb{R^d}$, the maxout layer implements the function:</li>
                                    </ul>
                            </ul>
                            <div class="equation"> $h_{i}(x) = \max\limits_{j \in [1, k]}^{} z_{i}^{(j)}$ </div>
                            <p style="padding-left:3em" >where:</p>
                            <div class="equation">
                                $z_{i}^{(j)} = x^{T}W_{i}^{(j)} + b_{i}^{(j)}$
                            </div>
                            <p style="padding-left:3em" >and:</p>
                            <div class="equation">
                                $W \in \mathbb{R}^{d \times m \times k} \text{and  } b \in \mathbb{R}^{m \times k}$
                            </div>
                            <ul>
                                <ul>
                                    <ul>
                                        <li>$m$: number of hidden units;</li>
                                        <li>$d$: size of input vector;</li>
                                        <li>$k$: number of linear models.</li>
                                    </ul>
                                </ul>
                            </ul>

                        </div>
                    </section>

                    <section>
                        <h2>Maxout</h2>
                        <div class="subtitle">Activation function</div>
                        <div class="div-alone">
                            <ul>
                                <li>Maxout learn the activation function:</li>
                                <ul>
                                    <li>with $k=2$ (number of maxout unit) the activation function can be:</li>
                                    <ul>
                                        <li>ReLU function;</li>
                                        <li>Absolute function;</li>
                                    </ul>
                                    <li>with $k=5$ the activation function can be a <i>quadratic</i> function.</li>
                                </ul>
                            </ul>
                        </div>
                        <div style="margin-top: 40px">
                            <img src="./img/maxout_net.png" alt="unifi stemma" width="50%">
                            <p class="img-note">Figure: example of maxout net with $2$ hidden units.</p>
                        </div>
                    </section>

                    <section>
                        <h2>Maxout</h2>
                        <div class="subtitle">Universal approximator</div>
                        <div class="div-alone">
                            <ul>
                                <li><b>Theorem (<i>universal approximator</i>)</b>: any continuous function $f$ can be approximated arbitrarily
                                    well on a compact domain $C ⊂ \mathbb{R}^{n}$ by a maxout network with two maxout hidden units.</li>

                                <div style="margin-left: 3em; margin-top: 1em">
                                    <b>Proof</b>:
                                    <ul>
                                        <li><i>(Wang, 2004)</i> any continuous function can be expressed as a difference of $2$ convex functions:</li>
                                    </ul>
                                </div>
                                <div class="equation">
                                    $g(x)=h_{1}(x)-h_{2}(x)$
                                </div>
                                <div style="margin-left: 3em;">
                                    <ul>
                                        <li><i>(Stone-Weierstrass)</i> any continuous function can be approximated by a piecewise linear function</li>
                                    </ul>
                                </div>
                                <div class="equation">
                                    $|f(x)-g(x)|< \epsilon$
                                </div>
                            </ul>
                        </div>
                        <div style="margin-top: 20px">
                            <img src="./img/maxout.png" alt="unifi stemma" width="60%">
                            <p class="img-note">Figure: examples of approximation with maxout.</p>
                        </div>
                    </section>

                </section>

                <section>
                    <section>
                        <h2>Softmax</h2>
                        <div class="subtitle">Softmax layer</div>
                        <div class="div-alone">
                            <ul>
                                <li><b>Softmax regression</b> is a generalization of logistic regression:</li>
                                <ul>
                                    <li>where we want to handle multiple classes;</li>
                                </ul>
                                <li>In logistic regression the labels are binary: $y^{(i)} \in \{0,1\}$ ;</li>
                                <li>Softmax regression allows us to handle: $y^{(i)} \in \{1,\ldots,K\}$ :</li>
                                <ul>
                                    <li>where $K$ is the number of classes;</li>
                                </ul>
                                <li>Given a training set: $\{ (x^{(1)}, y^{(1)}), \ldots, (x^{(m)}, y^{(m)}) \}$ of $m$ labeled examples, where the input features are $x^{(i)} \in \mathbb{R}^n$.</li>
                                <ul>
                                    <li>with logistic regression the hypotesis took the form:</li>
                                </ul>
                            </ul>
                            <br>
                             <div align="center" style="margin-top: 1em; margin-bottom: 1em">$h_\theta(x) = \frac{1}{1+\exp(-\theta^\top x)}$</div>
                            <ul>
                                <ul>
                                    <li>where $\theta^\top$ are the parameters to optimize.</li>
                                </ul>
                            </ul>
                        </div>
                    </section>

                    <section>
                        <h2>Softmax</h2>
                        <div class="subtitle">Softmax layer</div>
                        <div class="div-alone">
                            <ul>
                                <li>Given a test input $x$, we want our hypothesis to estimate the probability that $P(y=k | x)$ for each value of $k = 1 \ldots, K$:</li>
                                <ul>
                                    <li>i.e., we want to estimate the probability of the class label taking on each of the $K$ different possible values.</li>
                                </ul>
                                <li>Thus, our hypothesis will output a $K$-dimensional vector (whose elements sum to 1) giving us our $K$ estimated probabilities:</li>
                                    <ul>
                                        <li>Concretely, our hypothesis $h_{\theta}(x)$ takes the form:</li>
                                    </ul>
                            </ul>
                            <div class="equation" style="margin-top: 1em; margin-bottom: 1em">
                               $
                                    \begin{align}
                                    h_\theta(x) =
                                    \begin{bmatrix}
                                    P(y = 1 | x; \theta) \\
                                    P(y = 2 | x; \theta) \\
                                    \vdots \\
                                    P(y = K | x; \theta)
                                    \end{bmatrix}
                                    =
                                    \frac{1}{ \sum_{j=1}^{K}{\exp(\theta^{(j)\top} x) }}
                                    \begin{bmatrix}
                                    \exp(\theta^{(1)\top} x ) \\
                                    \exp(\theta^{(2)\top} x ) \\
                                    \vdots \\
                                    \exp(\theta^{(K)\top} x ) \\
                                    \end{bmatrix}
                                    \end{align}
                                    $
                            </div>

                            <ul>
                                <ul>
                                    <li>where $\theta^{(1)}, \theta^{(2)}, \ldots, \theta^{(K)} \in \mathbb{R}^{n}$ are the parameters of our model:</li>
                                        <ul>
                                            <li><i>Notice</i> that the term $\frac{1}{ \sum_{j=1}^{K}{\exp(\theta^{(j)\top} x) } } $ normalizes the distribution, so that it sums to one.</li>
                                        </ul>
                                </ul>
                            </ul>
                        </div>
                    </section>

                    <section>
                        <h2>Cost Function</h2>
                        <div class="subtitle">Softmax cost</div>
                        <div class="div-alone">
                            <ul>
                                <li>The <b>cost function</b> used in softmax regression is the follow:</li>
                            </ul>
                            <div class="equation" style="margin-top: 1em; margin-bottom: 1em">
                                $ J(\theta) = - \left[ \sum_{i=1}^{m} \sum_{k=1}^{K}  1\left\{y^{(i)} = k\right\} \log \frac{\exp(\theta^{(k)\top} x^{(i)})}{\sum_{j=1}^K \exp(\theta^{(j)\top} x^{(i)})}\right] $
                            </div>
                            <ul>
                                <ul>
                                    <li>that is a generalization of logistic regression cost function:</li>
                                </ul>
                            </ul>
                            <div class="equation" style="margin-top: 1em; margin-bottom: 1em">
                                $J(\theta) = - \left[ \sum_{i=1}^m   (1-y^{(i)}) \log (1-h_\theta(x^{(i)})) + y^{(i)} \log h_\theta(x^{(i)}) \right]
= - \left[ \sum_{i=1}^{m} \sum_{k=0}^{1} 1\left\{y^{(i)} = k\right\} \log P(y^{(i)} = k | x^{(i)} ; \theta) \right]$
                            </div>

                            <ul>
                                <ul>
                                    <li>where we sum over the $K$ different possible values of the class label.</li>
                                </ul>
                                <li>To obtain the parameters $\theta$ we'll resort to an iterative optimization algorithm using the gradient:</li>
                            </ul>

                            <div class="equation" style="margin-top: 1em; margin-bottom: 1em">
$ \nabla_{\theta^{(k)}} J(\theta) = - \sum_{i=1}^{m}{ \left[ x^{(i)} \left( 1\{ y^{(i)} = k\}  - P(y^{(i)} = k | x^{(i)}; \theta) \right) \right]  } $
                            </div>

                        </div>
                    </section>

                </section>

                <section>

                    <section>
                        <h2>Optimization method</h2>
                        <div class="subtitle">Gradient Descent methods</div>
                        <div class="div-alone">
                            <ul>
                                <li>The Gradient Descent (GD) method is used to optimize the cost function:</li>
                                <ul>
                                    <li>called Batch GD which use the full training set to compute the next update to parameters at each iteration;</li>
                                    <li>in practice computing the cost and gradient for the entire training set can be:
                                        <ul>
                                            <li>very slow;</li>
                                            <li>sometimes intractable on a single machine if the dataset is too big to fit in main memory.</li>
                                        </ul>
                                </ul>
                                <li><b>Stochastic Gradient Descent (SGD)</b> addresses both of these issues by following the negative gradient of the objective
                                    after seeing:
                                <ul>
                                    <li>only a single or a few training examples: <b>online</b> setting;</li>
                                    <li>a small batch of examples e.g. $128$, $256$: <b>Minibatch SGD</b>.</li>
                                </ul>
                            </ul>
                        </div>
                    </section>

                    <section>
                        <h2>Optimization method</h2>
                        <div class="subtitle">Minibatch Stochastic Gradient Descent</div>
                        <div class="div-alone">
                            <ul>
                                <li>The standard gradient descent algorithm updates the parameters $\theta$ of the objective $J(\theta)$ as:</li>
                            </ul>
                            <div class="equation" style="margin-top: 1em; margin-bottom: 1em">
                                $\theta = \theta - \alpha \nabla_\theta E[J(\theta)]$
                            </div>

                            <ul>
                                <ul>
                                    <li>where the expectation in the above equation is approximated by evaluating the cost and gradient over the full training set.</li>
                                </ul>
                                <li>Minibatch SGD update and computes the gradient of the parameters using a small batch of examples;</li>
                                <ul>
                                    <li>the new update is given by:</li>
                                </ul>
                            </ul>

                            <div class="equation" style="margin-top: 1em; margin-bottom: 1em">
                                $\theta = \theta - \alpha \nabla_\theta J(\theta; x^{b},y^{b})$
                            </div>

                            <ul>
                                <ul>
                                    <li>where $\{x^{b}, y^{b}\}$ is the $b$-th subset of training set and $\alpha$ is the learning rate.</li>
                                </ul>
                            </ul>
                        </div>
                    </section>

                    <section>
                        <h2>Optimization method</h2>
                        <div class="subtitle">Momentum</div>
                        <div class="div-alone">
                            <ul>
                                <li>The objective has the form of a long shallow ravine leading to the optimum and steep walls on the sides, standard SGD will tend to oscillate across the narrow ravine since the negative gradient will point down one of the steep sides rather than along the ravine towards the optimum;</li>
                                <li><b>Momentum</b> is one method for pushing the objective more quickly along the shallow ravine;</li>
                                <ul>
                                    <li>The momentum update is given by:</li>
                                </ul>
                            </ul>
                            <div class="equation" style="margin-top: 1em; margin-bottom: 1em">
                                $\theta^{k+1} = \theta^{k} -  \Delta \theta^{k+1}$
                            </div>
                            <div class="equation" style="margin-top: 1em; margin-bottom: 1em">
                                $\Delta \theta^{k+1} = \gamma \Delta \theta^{k} + \alpha \nabla_{\theta} J(\theta; x^{(i)},y^{(i)})$
                            </div>

                            <ul>
                                <ul>
                                    <li>where $\gamma$ is the <i>momentum</i> and $\alpha$ the learning rate.</li>
                                </ul>
                            </ul>

                        </div>
                    </section>

                </section>

                <section>
                    <h2>Implementation technologies</h2>
                    <div class="subtitle">Tools</div>
                    <div class="div-alone">
                        <ul>
                            <li>Dataset: CIFAR-10</li>
                            <li>Python:</li>
                            <ul>
                                <li>Pylearn2</li>
                                <li>Theano</li>
                                <li>Nvidia CUDA</li>
                            </ul>
                            <li>Pycharm</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2>Multi-Column Convolutional Neural Network</h2>
                    <div class="subtitle">Architecture</div>
                    <div class="div-alone">
                        <ul>
                            <li>We will go through the implementation details.</li>
                        </ul>
                    </div>
                    <div>
                        <img src="./img/mcdnn_trasparent.png" alt="unifi stemma" >
                    </div>
                </section>

                <section>
                    <section>
                        <h2>Dataset</h2>
                        <div class="subtitle">Cifar-10</div>

                        <div class="div-left">
                            <br>
                            <ul>
                                <li>The CIFAR-10 dataset is a famous benchmark:</li>
                                <ul>
                                    <li>consists of $60000$ $32\times 32$ colour images in $10$ classes, with $6000$ images per class;</li>
                                    <li>there are $50000$ training images and $10000$ test images:</li>
                                    <ul>
                                        <li>the <b>test</b> batch contains $1000$ randomly-selected images from each class;</li>
                                        <li>the <b>training</b> batches contain the remaining images in random order</li>
                                    </ul>
                                </ul>
                            </ul>
                        </div>

                        <div class="div-right">
                            <br>
                            <img  width="80%" src="./img/cifar10.png" alt="cifar10">
                            <div class="img-note">Cifar-10 dataset.</div>
                        </div>

                    </section>

                    <section>
                        <h2>Dataset</h2>
                        <div class="subtitle">Preprocessing</div>

                        <div class="div-alone">
                            <ul>
                                <li>Operations that give as a result a modified image with the same dimensions as the
                                    original image (e.g., contrast enhancement and noise reduction).</li>
                                <li>In order to <b>enhance the performance</b> of the MCDNN, the dataset is preprocessed in $3$ different ways:</li>
                                <ul>
                                    <li><i>Global Contrast Normalization (GCN)</i>;</li>
                                    <li><i>ZCA Whitening</i>;</li>
                                    <li><i>Toronto preprocessing</i>.</li>
                                </ul>
                                <li>Each preprocessed dataset is given as input to a single DNN;</li>
                            </ul>
                        </div>
                    </section>

                    <section>
                        <h2>Dataset</h2>
                        <div class="subtitle">GCN and Toronto preprocessing</div>

                        <div class="div-alone">
                            <ul>
                                <li>In image processing, normalization is a process that changes the range of pixel intensity
                        values.</li>
                                <li><b>Global Contrast Normalization</b>:</li>
                                <ul>
                                    <li>each training sample is normalized by subtracting the per-example mean across pixels;</li>
                                    <li>and then normalizes by either the vector norm or the standard deviation;</li>
                                </ul>
                                <li><b>Toronto preprocessing</b>:</li>
                                <ul>
                                    <li>each training sample is normalized by subtracting the per-pixel mean across examples;</li>
                                    <li>and then normalizes by the maximum intensity value i.e. $255$;</li>
                                    <ul>
                                        <li>if the test is used the mean must be computed across the training set.</li>
                                    </ul>
                                </ul>
                            </ul>
                        </div>

                    </section>

                    <section>
                        <h2>Dataset</h2>
                        <div class="subtitle">Zero Components Analysis (ZCA)</div>

                        <div class="div-alone">

                        </div>

                        We can store $n$ dimensional data points in the columns of a $d \times n$ matrix $X$.
                        Assuming the data points have zero mean, their covariance matrix is given by

                        <div class="equation">
                            $\frac{1}{n-1}XX^T$.
                        </div>

                        We wish to decorrelate the data dimensions from one another. We can do this with a linear
                        transformation $W$, which will transform the data matrix $X$ as follows:

                        <div class="equation">
                            $Y=WX$.
                        </div>

                        In order for W to be a decorrelating matrix, $Y Y^T$ must be diagonal. However, we can
                        restrict our search only to $W$s that satisfy

                        <div class="equation">
                            $YY^T=(n-1)I$.
                        </div>

                        In other words, $W$s that make the covariance matrix of the transformed data matrix
                        equal to the identity. There are multiple $W$s that fit this description, so we can restrict
                        our search further by requiring

                        <div class="equation">
                            $W = W^T$.
                        </div>

                        Given these restrictions, we can find $W$:

                        <div class="equation">
                            $
                            \begin{split}
                            YY^T & = (n-1)I \\
                            WXX^TW^T & = (n-1)I \\
                            W^TWXX^TW^T &= (n-1)W^T \\
                            W^2XX^TW^T &= (n-1)W^T \\
                            W^2XX^T &= (n-1)I \\
                            W^2 &= (n-1)(XX^T)^{-1} \\
                            W &= \sqrt{n-1}(XX^T)^{-\frac{1}{2}}.
                            \end{split}$
                        </div>
                    </section>
                    <section>
                        <h2>ZCA Whitening</h2>

                        $(XX^T)^{-\frac{1}{2}}$ is easly found because $XX^T$ is symmetric and hence othogonally
                        diagonalizable.

                        <div class="equation">
                            $XX^T = PDP^T$
                        </div>

                        for some orthogonal matrix P and diagonal matrix D. So

                        <div class="equation">
                            $
                            \begin{split}
                            (XX^T)^{-\frac{1}{2}} & = ((XX^T)^{-1})^{\frac{1}{2}} \\
                            &= ((PDP^T)^{-1})^{\frac{1}{2}} \\
                            &= (PD^{-1}P^T)^{\frac{1}{2}} \\
                            &= PD^{-\frac{1}{2}}P^T
                            \end{split}$
                        </div>

                        where $D^{-\frac{1}{2}}$ is just $D$ with all the elements taken to the power $-\frac{1}{2}$.
                        <br>
                        Some $W = (XX^T)^{-\frac{1}{2}}$ transforms $X$ in such a way that the resultant data dimensions
                        are uncorrelated with one another and the variance in each dimension is exactly $1$.
                        $W$ may also be thought of as rotating $X$ to the space of its principal components, dividing
                        each principal component by the square root of the variance in that direction, and then rotating
                        back to pixel space. $W$ is called a <b>whitening matrix</b>, and is referred to as the
                        <i>Zero Components Analysis</i> (ZCA) solution to the equation

                        <div class="equation">
                            $YY^T = diagonal$
                        </div>

                        The <i>dewhitening matrix</i>, $W^{-1}$, is given by

                        <div class="equation">
                            $W^{-1} = PD^{\frac{1}{2}}P^T$.
                        </div>
                </section>
                    <section>
                        <h2>ZCA Whitening</h2>

                        <div class="div-left">
                            <img src="./img/pre_zca.png" alt="unifi stemma" >
                            <div class="img-note">Pre ZCA</div>
                        </div>

                        <div class="div-right">
                            <img src="./img/after_zca.png" alt="unifi stemma" >
                            <div class="img-note">After ZCA</div>
                        </div>

                        <br><br>
                        ZCA-whitened images still resemble normal images, whereas PCA-whitened ones look nothing
                        like normal images. This is probably important for algorithms like convolutional neural
                        networks, which treat neighbouring pixels together
                        and so greatly rely on the local properties of natural images. For most other machine
                        learning algorithms it should be absolutely irrelevant whether the data is whitened with
                        PCA or ZCA.

                    </section>

                </section>


                <section>
                    <section>
                        <h2>Single Convolutional Neural Network</h2>
                        <div class="subtitle">Architecture: Convolutional and Max-pooling layers</div>
                        <div class="div-left">
                            <div style="min-height: 100px; overflow: hidden"></div>
                            <pre><code>
!obj:pylearn2.models.maxout.MaxoutConvC01B {
    pad: 4,
    num_channels: 48,
    num_pieces: 2,
    kernel_shape: [8, 8],
    pool_shape: [4, 4],
    pool_stride: [2, 2],
    irange: .005,
},
					        </code></pre>

                            <div class="img-note">
                                Figure: example of YAML code for convolutional layer definition.
                            </div>

                            <!--<div style="min-height: 50px; overflow: hidden"></div>-->

                        </div>

                        <div class="div-right">
                            <div style="text-align: left; padding: 30px">
                                <ul>
                                    <li>The single architecture use $3$ convolutional layer $\{h_0, h_1, h_2\}$ with the following parameters:</li>
                                    <ul>
                                        <li><b>pad</b>: apply a padding of $n$ pixel:</li>
                                        <ul>
                                            <li>$4$, $3$, $3$</li>
                                        </ul>
                                        <li><b>num_channels</b>: is the number of feature maps:</li>
                                        <ul>
                                            <li>$48$, $128$, $128$</li>
                                        </ul>
                                        <li><b>num_pieces</b>: use $n$ linear models for each maxout unit;</li>
                                        <ul>
                                            <li>$2$, $2$, $2$</li>
                                        </ul>
                                        <li><b>kernel_shape</b>:</li>
                                        <ul>
                                            <li>$8\times8$, $8\times8$, $5\times5$</li>
                                        </ul>
                                       <li><b>pool_shape</b>:</li>
                                        <ul>
                                            <li>$4\times4$, $4\times4$, $2\times2$</li>
                                        </ul>
                                        <li><b>pool_stride</b>:</li>
                                        <ul>
                                            <li>$2\times2$, $2\times2$, $2\times2$</li>
                                        </ul>
                                    </ul>
                                </ul>
                            </div>
                        </div>
                    </section>

                    <section>
                        <h2>Single Convolutional Neural Network</h2>
                        <div class="subtitle">Architecture: Fully connected layer</div>
                        <div class="div-left">
                            <div style="min-height: 50px; overflow: hidden"></div>
                            <pre><code>
!obj:pylearn2.models.maxout.Maxout {
    layer_name: 'h3',
    irange: .005,
    num_units: 240,
    num_pieces: 5,
},
					        </code></pre>

                            <div class="img-note">
                                Figure: example of YAML code for fully connected layer.
                            </div>

                            <!--<div style="min-height: 50px; overflow: hidden"></div>-->

                        </div>

                        <div class="div-right">
                            <div style="text-align: left; padding: 30px">
                                <ul>
                                    <li>The single architecture use a fully connected layer $h_3$ with the following parameters:</li>
                                    <ul>
                                        <li><b>num_units</b>: use $240$ neurons with maxout activation function;</li>
                                        <li><b>num_pieces</b>: use $5$ linear models for each maxout unit;</li>
                                    </ul>
                                </ul>
                            </div>
                        </div>
                    </section>


                    <section>
                        <h2>Single Convolutional Neural Network</h2>
                        <div class="subtitle">Architecture: Softmax layer</div>
                        <!--<div class="div-header">In training fase</div>-->


                        <div class="div-left">
                            <div style="min-height: 50px; overflow: hidden"></div>
                            <pre><code>
!obj:pylearn2.models.mlp.Softmax {
    layer_name: 'y',
    n_classes: 10,
    irange: .005
}
					        </code></pre>

                            <div class="img-note">
                                Figure: example of YAML code for softmax layer.
                            </div>
                        </div>

                        <div class="div-right">
                            <div style="text-align: left; padding: 30px">
                                <ul>
                                    <li>The single architecture use a softmax layer $y$ with the following parameters:</li>
                                    <ul>
                                        <li><b>n_classes</b>: use $10$ output class vector;</li>
                                    </ul>
                                </ul>
                            </div>
                        </div>
                    </section>

                    <section>
                        <h2>Single Convolutional Neural Network</h2>
                        <div class="subtitle">Training algorithm</div>

                        <div>
                            <pre><code>
algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
    learning_rate: .1,
    learning_rule: !obj:pylearn2.training_algorithms.learning_rule.Momentum {
        init_momentum: .5,
    },
    cost: !obj:pylearn2.costs.mlp.dropout.Dropout {
        input_include_probs: { 'h0' : 0.8 },
    },
    termination_criterion: !obj:pylearn2.termination_criteria.MonitorBased {
        channel_name: "valid_y_misclass",
        prop_decrease: 0.,
        N: 100
    },
},
					        </code></pre>

                            <div class="img-note">
                                Figure: example of YAML code for softmax layer.
                            </div>
                        </div>

                        <div class="div-alone" style="padding-left: 10px">
                            The learning algorithm used is Minibatch SGD with the following parameters:
                            <ul style="padding-left: 40px; padding-top: 10px">
                                <li><b>Dropout</b>: apply dropout to the layer $h_0$ with $0.8$ probability;</li>
                                <li><b>Termination criterion</b>: stop if miss class rate doesn't decrease in $100$ iterations;</li>
                                <!--<ul>-->
                                    <!--<li></li>-->
                                <!--</ul>-->
                            </ul>
                        </div>

                        <!--<div class="div-right">-->
                            <!--<ul>-->
                                <!--<li><b>Termination criterion</b>: stop if miss class rate doesn't decrease in $100$ iterations;</li>-->
                                <!--<ul>-->
                                    <!--<li></li>-->
                                <!--</ul>-->
                            <!--</ul>-->
                        <!--</div>-->
                    </section>
                </section>

                <section>
                    <section>
                        <h2>Architettura MCDNN: passiamo al Multicolumn...</h2>
                        <p>
                            Idea: blablabla
                        </p>
                    </section>
                    <section>
                        <h2>Multi-Column Naive:</h2>
                        <p>
                            immagine + spiegazione
                        </p>
                    </section>
                    <section>
                        <h2>Multi-Column vero</h2>
                        <p>
                            immagine + spiegazione
                        </p>
                    </section>
                </section>

                <section>
                    <section>
                        <h2>Esperimento</h2>
                        <p>
                            blablabla
                        </p>
                    </section>
                    <section>
                        <h2>Singola rete</h2>
                        <p>
                            parametri blablabla
                        </p>
                    </section>
                    <section>
                        <h2>MultiColumn</h2>
                        <p>
                            blablabla
                        </p>
                    </section>
                    <section>
                        <h2>VectorSpacesDataset</h2>
                        <p>
                            blablabla
                        </p>
                    </section>
                </section>
                <section>
                    <section>
                        <h2>Risultati</h2>
                        <p>
                            <!--_____Results __________	______________________-->
                            <!--_______METHOD__________	_____MEAN____VAR______-->
                            <!--Single GCN             	  (0.189, 0.15327900000000005)-->
                            <!--Single TOR             	  (0.2036, 0.16214703999999996)-->
                            <!--Single ZCA             	  (0.15140000000000001, 0.12847804000000002)-->
                            <!---------------------------------------------------------------------->
                            <!--Multi GCN_TOR          	  (0.1701, 0.14116598999999994)-->
                            <!--Multi GCN_ZCA          	  (0.14960000000000001, 0.12721984)-->
                            <!--Multi ZCA_TOR          	  (0.13300000000000001, 0.11531099999999997)-->
                            <!--Multi GCN_TOR_ZCA      	  (0.14810000000000001, 0.12616639000000004)-->
                            <!----------------------------------------------------------------------->
                            <!--Multi-Naive GCN_TOR    	  (0.18010000000000001, 0.14766398999999997)-->
                            <!--Multi-Naive GCN_ZCA    	  (0.15179999999999999, 0.12875676)-->
                            <!--Multi-Naive ZCA_TOR    	  (0.1424, 0.12212224000000001)-->
                            <!--Multi-Naive GCN_TOR_ZCA	  (0.154, 0.13028399999999998)-->
                            <!--_______________________________________-->
                        <br>
                        <div class="datagrid"><table>
                            <thead>
                                <tr><th width="40%">Method</th><th width="30%">Mean Error</th><th width="30%">Var</th></tr>
                            </thead>
                            <tbody>
                                <tr class="alt"><td>Single GCN</td><td>0.189</td><td>0.153279</td></tr>
                                <tr><td>Single TOR</td><td>0.2036</td><td>0.162147</td></tr>
                                <tr class="alt"><td>Single ZCA</td><td>0.1514</td><td>0.128478</td></tr>

                                <!--<tr><td>Multi-Naive GCN_TOR</td><td>0.1801</td><td>0.147663</td></tr>-->
                                <!--<tr class="alt"><td>Multi-Naive GCN_ZCA</td><td>0.151799</td><td>0.128756</td></tr>-->
                                <!--<tr><td>Multi-Naive ZCA_TOR </td><td>0.1424</td><td>0.122122</td></tr>-->
                                <!--<tr class="alt"><td>Multi-Naive GCN_TOR_ZCA</td><td>0.154</td><td>0.130283</td></tr>-->

                                <!--<tr><td>Multi GCN_TOR</td><td>0.1701</td><td>0.141165</td></tr>-->
                                <!--<tr class="alt"><td>Multi GCN_ZCA</td><td>0.1496</td><td>0.127219</td></tr>-->
                                <!--<tr><td>Multi ZCA_TOR</td><td>0.1330</td><td>0.115310</td></tr>-->
                                <!--<tr class="alt"><td>Multi GCN_TOR_ZCA</td><td>0.14810</td><td>0.126166</td></tr>-->
                            </tbody>
                            </table>
                        </div>
                        <div class="img-note">Tab 1. Single Column Deep Neural Network</div>
                        <br>
                        <div class="datagrid"><table>
                            <thead>
                                <tr><th width="40%">Method</th><th width="30%">Mean Error</th><th width="30%">Var</th></tr>
                            </thead>
                            <tbody>

                                <tr><td>Multi-Naive GCN_TOR</td><td>0.1801</td><td>0.147663</td></tr>
                                <tr class="alt"><td>Multi-Naive GCN_ZCA</td><td>0.151799</td><td>0.128756</td></tr>
                                <tr><td>Multi-Naive ZCA_TOR </td><td>0.1424</td><td>0.122122</td></tr>
                                <tr class="alt"><td>Multi-Naive GCN_TOR_ZCA</td><td>0.154</td><td>0.130283</td></tr>

                            </tbody>
                            </table>
                        </div>
                        <div class="img-note">Tab 2. Multi Column Deep Neural Network, naive implementation</div>
                        <br>
                        <div class="datagrid"><table>
                            <thead>
                                <tr><th width="40%">Method</th><th width="30%">Mean Error</th><th width="30%">Var</th></tr>
                            </thead>
                            <tbody>

                                <tr><td>Multi GCN_TOR</td><td>0.1701</td><td>0.141165</td></tr>
                                <tr class="alt"><td>Multi GCN_ZCA</td><td>0.1496</td><td>0.127219</td></tr>
                                <tr><td>Multi ZCA_TOR</td><td>0.1330</td><td>0.115310</td></tr>
                                <tr class="alt"><td>Multi GCN_TOR_ZCA</td><td>0.14810</td><td>0.126166</td></tr>
                            </tbody>
                            </table>
                        </div>
                        <div class="img-note">Tab 3. Multi Column Deep Neural Network</div>


                        </p>
                    </section>
                    <section>
                        <h2>Confusion Matrix</h2>
                        <p>
                            <div class="">
                            <img src="./img/confusionMulti_ZCA_TOR.png" alt="unifi stemma" >
                            <div class="img-note">Confusion Matrix of a MCDNN: ZCA, TOR</div>
                        </div>
                        </p>
                    </section>
                    <section>
                        <h2>Confronto Paper</h2>
                        <p>
                            blablabla
                        </p>
                    </section>
                </section>

                <section>
                    <h2>References</h2>
                    <div id="div-alone">
                        <!--<object type="text/html" data="references.html">-->
                        <!--<p>backup content</p>-->
                        <!--</object>-->
                        <ul>
                          <li>[1] D. Cireșan, U. Meier and J. Schmidhuber: Multi-column Deep Neural Networks for Image Classification (2012);</li>
                          <li>[2] D. H. Hubel and T. N. Wiesel: Receptive fields, binocular interaction and functional architecture in the cat's visual cortex (1962);</li>
                          <li>[3] K. Fukushima: Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position (1980);</li>
                          <li>[4] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever and R. R. Salakhutdinov: Improving neural networks by preventing co-adaptation of feature detectors (2012);</li>
                            <li>[5] X. Glorot, A. Bordes and Y. Bengio: Deep sparse rectifier neural networks (2011);</li>
                            <li>[6] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville and Y. Bengio: Maxout Networks (2013)</li>
                        </ul>
                    </div>
                </section>












				<section>
					<h2>Heads Up</h2>
					<p>
						reveal.js is a framework for easily creating beautiful presentations using HTML. You'll need a browser with
						support for CSS 3D transforms to see it in its full glory.
					</p>

					<aside class="notes">
						Oh hey, these are some notes. They'll be hidden in your presentation, but you can see them if you open the speaker notes window (hit 's' on your keyboard).
					</aside>
				</section>

				<!-- Example of nested vertical slides -->
				<section>
					<section>
						<h2>Vertical Slides</h2>
						<p>
							Slides can be nested inside of other slides,
							try pressing <a href="#" class="navigate-down">down</a>.
						</p>
						<a href="#" class="image navigate-down">
							<img width="178" height="238" src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
						</a>
					</section>
					<section>
						<h2>Basement Level 1</h2>
						<p>Press down or up to navigate.</p>
					</section>
					<section>
						<h2>Basement Level 2</h2>
						<p>Cornify</p>
						<a class="test" href="http://cornify.com">
							<img width="280" height="326" src="https://s3.amazonaws.com/hakim-static/reveal-js/cornify.gif" alt="Unicorn">
						</a>
					</section>
					<section>
						<h2>Basement Level 3</h2>
						<p>That's it, time to go back up.</p>
						<a href="#/2" class="image">
							<img width="178" height="238" src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Up arrow" style="-webkit-transform: rotate(180deg);">
						</a>
					</section>
				</section>

				<section>
					<h2>Slides</h2>
					<p>
						Not a coder? No problem. There's a fully-featured visual editor for authoring these, try it out at <a href="http://slid.es" target="_blank">http://slid.es</a>.
					</p>
				</section>

				<section>
					<h2>Point of View</h2>
					<p>
						Press <strong>ESC</strong> to enter the slide overview.
					</p>
					<p>
						Hold down alt and click on any element to zoom in on it using <a href="http://lab.hakim.se/zoom-js">zoom.js</a>. Alt + click anywhere to zoom back out.
					</p>
				</section>

				<section>
					<h2>Works in Mobile Safari</h2>
					<p>
						Try it out! You can swipe through the slides and pinch your way to the overview.
					</p>
				</section>

				<section>
					<h2>Marvelous Unordered List</h2>
					<ul>
						<li>No order here</li>
						<li>Or here</li>
						<li>Or here</li>
						<li>Or here</li>
					</ul>
				</section>

				<section>
					<h2>Fantastic Ordered List</h2>
					<ol>
						<li>One is smaller than...</li>
						<li>Two is smaller than...</li>
						<li>Three!</li>
					</ol>
				</section>

				<section data-markdown>
					<script type="text/template">
						## Markdown support

						For those of you who like that sort of thing. Instructions and a bit more info available [here](https://github.com/hakimel/reveal.js#markdown).

						```
						<section data-markdown>
						  ## Markdown support

						  For those of you who like that sort of thing.
						  Instructions and a bit more info available [here](https://github.com/hakimel/reveal.js#markdown).
						</section>
						```
					</script>
				</section>

				<section id="transitions">
					<h2>Transition Styles</h2>
					<p>
						You can select from different transitions, like: <br>
						<a href="?transition=cube#/transitions">Cube</a> -
						<a href="?transition=page#/transitions">Page</a> -
						<a href="?transition=concave#/transitions">Concave</a> -
						<a href="?transition=zoom#/transitions">Zoom</a> -
						<a href="?transition=linear#/transitions">Linear</a> -
						<a href="?transition=fade#/transitions">Fade</a> -
						<a href="?transition=none#/transitions">None</a> -
						<a href="?#/transitions">Default</a>
					</p>
				</section>

				<section id="themes">
					<h2>Themes</h2>
					<p>
						Reveal.js comes with a few themes built in: <br>
						<a href="?#/themes">Default</a> -
						<a href="?theme=sky#/themes">Sky</a> -
						<a href="?theme=beige#/themes">Beige</a> -
						<a href="?theme=simple#/themes">Simple</a> -
						<a href="?theme=serif#/themes">Serif</a> -
						<a href="?theme=night#/themes">Night</a> <br>
						<a href="?theme=moon#/themes">Moon</a> -
						<a href="?theme=solarized#/themes">Solarized</a>
					</p>
					<p>
						<small>
							* Theme demos are loaded after the presentation which leads to flicker. In production you should load your theme in the <code>&lt;head&gt;</code> using a <code>&lt;link&gt;</code>.
						</small>
					</p>
				</section>

				<section>
					<h2>Global State</h2>
					<p>
						Set <code>data-state="something"</code> on a slide and <code>"something"</code>
						will be added as a class to the document element when the slide is open. This lets you
						apply broader style changes, like switching the background.
					</p>
				</section>

				<section data-state="customevent">
					<h2>Custom Events</h2>
					<p>
						Additionally custom events can be triggered on a per slide basis by binding to the <code>data-state</code> name.
					</p>
					<pre><code data-trim contenteditable style="font-size: 18px; margin-top: 20px;">
Reveal.addEventListener( 'customevent', function() {
	console.log( '"customevent" has fired' );
} );
					</code></pre>
				</section>

				<section>
					<section data-background="#007777">
						<h2>Slide Backgrounds</h2>
						<p>
							Set <code>data-background="#007777"</code> on a slide to change the full page background to the given color. All CSS color formats are supported.
						</p>
						<a href="#" class="image navigate-down">
							<img width="178" height="238" src="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" alt="Down arrow">
						</a>
					</section>
					<section data-background="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png">
						<h2>Image Backgrounds</h2>
						<pre><code>&lt;section data-background="image.png"&gt;</code></pre>
					</section>
					<section data-background="https://s3.amazonaws.com/hakim-static/reveal-js/arrow.png" data-background-repeat="repeat" data-background-size="100px">
						<h2>Repeated Image Backgrounds</h2>
						<pre><code style="word-wrap: break-word;">&lt;section data-background="image.png" data-background-repeat="repeat" data-background-size="100px"&gt;</code></pre>
					</section>
				</section>

				<section data-transition="linear" data-background="#4d7e65" data-background-transition="slide">
					<h2>Background Transitions</h2>
					<p>
						Pass reveal.js the <code>backgroundTransition: 'slide'</code> config argument to make backgrounds slide rather than fade.
					</p>
				</section>

				<section data-transition="linear" data-background="#8c4738" data-background-transition="slide">
					<h2>Background Transition Override</h2>
					<p>
						You can override background transitions per slide by using <code>data-background-transition="slide"</code>.
					</p>
				</section>

				<section>
					<h2>Clever Quotes</h2>
					<p>
						These guys come in two forms, inline: <q cite="http://searchservervirtualization.techtarget.com/definition/Our-Favorite-Technology-Quotations">
						&ldquo;The nice thing about standards is that there are so many to choose from&rdquo;</q> and block:
					</p>
					<blockquote cite="http://searchservervirtualization.techtarget.com/definition/Our-Favorite-Technology-Quotations">
						&ldquo;For years there has been a theory that millions of monkeys typing at random on millions of typewriters would
						reproduce the entire works of Shakespeare. The Internet has proven this theory to be untrue.&rdquo;
					</blockquote>
				</section>

				<section>
					<h2>Pretty Code</h2>
					<pre><code data-trim contenteditable>
function linkify( selector ) {
  if( supports3DTransforms ) {

    var nodes = document.querySelectorAll( selector );

    for( var i = 0, len = nodes.length; i &lt; len; i++ ) {
      var node = nodes[i];

      if( !node.className ) {
        node.className += ' roll';
      }
    }
  }
}
					</code></pre>
					<p>Courtesy of <a href="http://softwaremaniacs.org/soft/highlight/en/description/">highlight.js</a>.</p>
				</section>

				<section>
					<h2>Intergalactic Interconnections</h2>
					<p>
						You can link between slides internally,
						<a href="#/2/3">like this</a>.
					</p>
				</section>

				<section>
					<section id="fragments">
						<h2>Fragmented Views</h2>
						<p>Hit the next arrow...</p>
						<p class="fragment">... to step through ...</p>
						<ol>
							<li class="fragment"><code>any type</code></li>
							<li class="fragment"><em>of view</em></li>
							<li class="fragment"><strong>fragments</strong></li>
						</ol>

						<aside class="notes">
							This slide has fragments which are also stepped through in the notes window.
						</aside>
					</section>
					<section>
						<h2>Fragment Styles</h2>
						<p>There's a few styles of fragments, like:</p>
						<p class="fragment grow">grow</p>
						<p class="fragment shrink">shrink</p>
						<p class="fragment roll-in">roll-in</p>
						<p class="fragment fade-out">fade-out</p>
						<p class="fragment highlight-red">highlight-red</p>
						<p class="fragment highlight-green">highlight-green</p>
						<p class="fragment highlight-blue">highlight-blue</p>
						<p class="fragment current-visible">current-visible</p>
						<p class="fragment highlight-current-blue">highlight-current-blue</p>
					</section>
				</section>

				<section>
					<h2>Spectacular image!</h2>
					<a class="image" href="http://lab.hakim.se/meny/" target="_blank">
						<img width="320" height="299" src="http://s3.amazonaws.com/hakim-static/portfolio/images/meny.png" alt="Meny">
					</a>
				</section>

				<section>
					<h2>Export to PDF</h2>
					<p>Presentations can be <a href="https://github.com/hakimel/reveal.js#pdf-export">exported to PDF</a>, below is an example that's been uploaded to SlideShare.</p>
					<iframe id="slideshare" src="http://www.slideshare.net/slideshow/embed_code/13872948" width="455" height="356" style="margin:0;overflow:hidden;border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen> </iframe>
					<script>
						document.getElementById('slideshare').attributeName = 'allowfullscreen';
					</script>
				</section>

				<section>
					<h2>Take a Moment</h2>
					<p>
						Press b or period on your keyboard to enter the 'paused' mode. This mode is helpful when you want to take distracting slides off the screen
						during a presentation.
					</p>
				</section>

				<section>
					<h2>Stellar Links</h2>
					<ul>
						<li><a href="http://slid.es">Try the online editor</a></li>
						<li><a href="https://github.com/hakimel/reveal.js">Source code on GitHub</a></li>
						<li><a href="http://twitter.com/hakimel">Follow me on Twitter</a></li>
					</ul>
				</section>

				<section>
					<h1>THE END</h1>
					<h3>BY Hakim El Hattab / hakim.se</h3>
				</section>

			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.min.js"></script>

		<script>

			// Full list of configuration options available here:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({

                // Display controls in the bottom right corner
                controls: true,

                // Display a presentation progress bar
                progress: true,

                // Display the page number of the current slide
                slideNumber: true,

                // Push each slide change to the browser history
                history: true,

                // Enable keyboard shortcuts for navigation
                keyboard: true,

                // Enable the slide overview mode
                overview: true,

                // Vertical centering of slides
                center: true,

                // Enables touch navigation on devices with touch input
                touch: true,

                // Loop the presentation
                loop: false,

                // Change the presentation direction to be RTL
                rtl: false,

                // Turns fragments on and off globally
                fragments: true,

                // Flags if the presentation is running in an embedded mode,
                // i.e. contained within a limited portion of the screen
                embedded: false,

                // Number of milliseconds between automatically proceeding to the
                // next slide, disabled when set to 0, this value can be overwritten
                // by using a data-autoslide attribute on your slides
                autoSlide: 0,

                // Stop auto-sliding after user input
                autoSlideStoppable: true,

                // Enable slide navigation via mouse wheel
                mouseWheel: true,

                // Hides the address bar on mobile devices
                hideAddressBar: true,

                // Opens links in an iframe preview overlay
                previewLinks: false,

                // Transition style
                transition: 'default', // default/cube/page/concave/zoom/linear/fade/none

                // Transition speed
                transitionSpeed: 'default', // default/fast/slow

                // Transition style for full page slide backgrounds
                backgroundTransition: 'default', // default/none/slide/concave/convex/zoom

                // Number of slides away from the current that are visible
                viewDistance: 3,

                // Parallax background image
                parallaxBackgroundImage: '', // e.g. "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'"

                // Parallax background size
                parallaxBackgroundSize: '', // CSS syntax, e.g. "2100px 900px"

				theme: Reveal.getQueryHash().theme, // available themes are in /css/theme

				// Parallax scrolling
				// parallaxBackgroundImage: 'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg',
				// parallaxBackgroundSize: '2100px 900px',

                math: {
                    mathjax: '../../mathjax/MathJax.js',
                    config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
                },

                dependencies: [
                    // Cross-browser shim that fully implements classList - https://github.com/eligrey/classList.js/
                    { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },

                    // Interpret Markdown in <section> elements
                    { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                    { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },

                    // Syntax highlight for <code> elements
                    { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },

                    // Zoom in and out with Alt+click
                    { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },

                    // Speaker notes
                    { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },

                    // Remote control your reveal.js presentation using a touch device
                    //{ src: 'plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } },

                    // MathJax
                    { src: 'plugin/math/math.js', async: true }
                ]
			});

		</script>

	</body>
</html>